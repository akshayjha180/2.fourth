1. Explain hadoop in layman's term
Hadoop Components -:
1. HDFS                                      2.MAP REDUCE
                                              • PIG
                                           	  • HIVE
 
 Hadoop is a collection of open source programs/procedures relating to Big Data analysis.
 Being open source, it is freely available for use, reuse and modification (with some restrictions)
 for anyone who is interested in it. Big Data scientists call Hadoop the ‘backbone’ of their
 operations. In fact, Hadoop certification has become the next stepping stone for experienced
 software professionals who are starting to feel stagnated in their current stream.
 The birth of Hadoop -:
 As it had always been with the IT industry, it was the innovation of a bunch of forward thinking
 software engineers at Apache Software Foundation that led to the introduction of Hadoop. These 
 engineers realized that reading data from bulk storage devices took longer than reading it from 
 small storage devices of multiple numbers working simultaneously. Moreover, eliminating one single
 large storage location also made data available to multiple users spread across a network.
 The first version of ApacheTM Hadoop framework was released in 2005 and ever since it paved the way
 for better Big Data Analytics. Internationally, Hadoop courses are in hot demand since they offer 
 the promise of a lucrative career in a domain, i.e. Big Data, which is going to grow with leaps and 
 bounds.

2. Explain the components of Hadoop framework

The demand for Big data Hadoop training courses has increased after Hadoop made a special showing in
various enterprises for big data management in a big way.Big data hadoop training course that deals
with the implementation of various industry use cases is necessary Understand how the hadoop ecosystem 
works to master Apache Hadoop skills and gain in-depth knowledge of big data ecosystem and hadoop
architecture.However, before you enroll for any big data hadoop training course it is necessary to 
get some basic idea on how the hadoop ecosystem works.
All the components of the Hadoop ecosystem, as explicit entities are evident. The holistic view of
Hadoop architecture gives prominence to Hadoop common, Hadoop YARN, Hadoop Distributed File Systems 
(HDFS) and Hadoop MapReduce of the Hadoop Ecosystem. Hadoop common provides all Java libraries, utilities,
OS level abstraction, necessary Java files and script to run Hadoop, while Hadoop YARN is a framework 
for job scheduling and cluster resource management.  HDFS in Hadoop architecture provides high throughput
access to application data and Hadoop MapReduce provides YARN based parallel processing of large data sets.

3. Explain the reasons to learn Big data technologies

• Demand for Big Data skills is extremely high, and being able to prove your
  expertise is of essence
• 64% of IT hiring managers rate skilled big data knowledge as having
  extremely high or high value when rating expertise of candidates; this is
  based on a survey by CompTIA.
• According to Forbes, the median advertised salary for professionals with
  Big Data expertise is $124,000 a year.
• IBM, Cisco, and Oracle together advertised 26,488 open positions that
  required Big Data expertise in the last twelve months.











